{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RSNA 2025 Intracranial Aneurysm Detection - Result Analysis\n",
        "\n",
        "This notebook performs comprehensive cross-fold analysis of misclassifications from the 5-fold CV training.\n",
        "\n",
        "## Analysis Framework\n",
        "- **Out-of-Fold (OOF) Predictions**: Uses 4-fold ensemble for each sample (no data leakage)\n",
        "- **Per-Class Analysis**: Detailed misclassification analysis for each of the 14 classes\n",
        "- **Hard Sample Identification**: Identifies different types of challenging cases\n",
        "- **Fold Agreement Analysis**: Analyzes consistency between fold predictions\n",
        "\n",
        "## Key Features\n",
        "- Comprehensive misclassification analysis\n",
        "- Per-class error breakdown\n",
        "- Hard sample case studies\n",
        "- Interactive visualizations\n",
        "- Actionable insights for model improvement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, average_precision_score\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append('..')\n",
        "from train import HybridAneurysmModel, Config\n",
        "from utils import LABEL_COLS, ID_COL, load_cached_volume, take_window, valid_coords\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "EXPERIMENT_DIR = \"../models/2025-09-11-20-34-47\"\n",
        "NUM_FOLDS = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load experiment configuration\n",
        "with open(f\"{EXPERIMENT_DIR}/used_config.yaml\", 'r') as f:\n",
        "    import yaml\n",
        "    config_dict = yaml.safe_load(f)\n",
        "\n",
        "# Create Config object\n",
        "config = Config()\n",
        "config.architecture = config_dict['model']['architecture']\n",
        "config.img_size = config_dict['data']['img_size']\n",
        "config.window_offsets = config_dict['data']['window_offsets']\n",
        "config.roi_box_fraction = config_dict['data']['roi_box_fraction']\n",
        "config.cache_dir = config_dict['paths']['cache_dir']\n",
        "config.device = DEVICE\n",
        "\n",
        "print(f\"Experiment: {EXPERIMENT_DIR}\")\n",
        "print(f\"Architecture: {config.architecture}\")\n",
        "print(f\"Image size: {config.img_size}\")\n",
        "print(f\"Device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Fold Analysis Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossFoldAnalyzer:\n",
        "    \"\"\"Comprehensive cross-fold analysis for misclassification detection\"\"\"\n",
        "    \n",
        "    def __init__(self, experiment_dir: str, config: Config):\n",
        "        self.experiment_dir = experiment_dir\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.num_classes = len(LABEL_COLS)\n",
        "        \n",
        "        # Data storage\n",
        "        self.fold_assignments = {}\n",
        "        self.oof_predictions = {}  # {sample_id: [pred1, pred2, pred3, pred4]}\n",
        "        self.true_labels = {}      # {sample_id: true_label_vector}\n",
        "        self.fold_models = {}\n",
        "        \n",
        "        # Analysis results\n",
        "        self.results = {}\n",
        "        \n",
        "    def load_fold_assignments(self, train_csv_path: str):\n",
        "        \"\"\"Load which fold each sample belongs to\"\"\"\n",
        "        df = pd.read_csv(train_csv_path)\n",
        "        self.fold_assignments = dict(zip(df[ID_COL], df['fold']))\n",
        "        print(f\"Loaded fold assignments for {len(self.fold_assignments)} samples\")\n",
        "        \n",
        "    def load_fold_models(self):\n",
        "        \"\"\"Load all fold models\"\"\"\n",
        "        for fold in range(NUM_FOLDS):\n",
        "            model_path = f\"{self.experiment_dir}/tf_efficientnet_b0_fold{fold}_best.pth\"\n",
        "            if os.path.exists(model_path):\n",
        "                model = HybridAneurysmModel(self.config)\n",
        "                state_dict = torch.load(model_path, map_location=self.device)\n",
        "                model.load_state_dict(state_dict)\n",
        "                model.to(self.device)\n",
        "                model.eval()\n",
        "                self.fold_models[fold] = model\n",
        "                print(f\"Loaded fold {fold} model\")\n",
        "            else:\n",
        "                print(f\"Warning: Model not found for fold {fold}\")\n",
        "        \n",
        "    def predict_sample(self, model, sample_id: str) -> np.ndarray:\n",
        "        \"\"\"Predict single sample using a model\"\"\"\n",
        "        try:\n",
        "            # Load cached volume\n",
        "            volume_path = f\"{self.config.cache_dir}/{sample_id}.npz\"\n",
        "            volume = load_cached_volume(volume_path)  # (N, H, W)\n",
        "            \n",
        "            # Prepare windows\n",
        "            N = volume.shape[0]\n",
        "            all_predictions = []\n",
        "            \n",
        "            # Process in batches\n",
        "            batch_size = 16\n",
        "            for i in range(0, N, batch_size):\n",
        "                batch_windows = []\n",
        "                batch_coords = []\n",
        "                \n",
        "                for center_idx in range(i, min(i + batch_size, N)):\n",
        "                    # Extract window\n",
        "                    window = take_window(volume, center_idx, self.config.window_offsets)\n",
        "                    \n",
        "                    # Convert to HWC and resize\n",
        "                    img_hwc = np.transpose(window, (1, 2, 0)).astype(np.float32)\n",
        "                    img_resized = cv2.resize(img_hwc, (self.config.img_size, self.config.img_size))\n",
        "                    \n",
        "                    # Convert to CHW tensor\n",
        "                    x_full = torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).float()\n",
        "                    x_roi = x_full.clone()  # Same for both streams (no coords available)\n",
        "                    coords = torch.zeros(2).float()  # No coordinates available\n",
        "                    \n",
        "                    batch_windows.append((x_full, x_roi, coords))\n",
        "                \n",
        "                # Stack and predict\n",
        "                if batch_windows:\n",
        "                    x_full_batch = torch.stack([x[0] for x in batch_windows]).to(self.device)\n",
        "                    x_roi_batch = torch.stack([x[1] for x in batch_windows]).to(self.device)\n",
        "                    coords_batch = torch.stack([x[2] for x in batch_windows]).to(self.device)\n",
        "                    \n",
        "                    with torch.no_grad():\n",
        "                        logits = model(x_full_batch, x_roi_batch, coords_batch)\n",
        "                        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "                        all_predictions.append(probs)\n",
        "            \n",
        "            if all_predictions:\n",
        "                # Aggregate across windows (max aggregation)\n",
        "                all_preds = np.vstack(all_predictions)\n",
        "                series_pred = all_preds.max(axis=0)\n",
        "                return series_pred\n",
        "            else:\n",
        "                return np.zeros(self.num_classes, dtype=np.float32)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting {sample_id}: {e}\")\n",
        "            return np.zeros(self.num_classes, dtype=np.float32)\n",
        "    \n",
        "    def collect_oof_predictions(self, sample_ids: List[str]):\n",
        "        \"\"\"Collect out-of-fold predictions for all samples\"\"\"\n",
        "        print(f\"Collecting OOF predictions for {len(sample_ids)} samples...\")\n",
        "        \n",
        "        for sample_id in tqdm(sample_ids):\n",
        "            sample_fold = self.fold_assignments.get(sample_id, -1)\n",
        "            if sample_fold == -1:\n",
        "                continue\n",
        "                \n",
        "            oof_predictions = []\n",
        "            \n",
        "            # Get predictions from the 4 folds that didn't train on this sample\n",
        "            for fold in range(NUM_FOLDS):\n",
        "                if fold != sample_fold and fold in self.fold_models:\n",
        "                    model = self.fold_models[fold]\n",
        "                    prediction = self.predict_sample(model, sample_id)\n",
        "                    oof_predictions.append(prediction)\n",
        "            \n",
        "            # Store predictions\n",
        "            if oof_predictions:\n",
        "                self.oof_predictions[sample_id] = oof_predictions\n",
        "    \n",
        "    def create_oof_ensemble(self, sample_id: str, method: str = \"mean\") -> np.ndarray:\n",
        "        \"\"\"Create ensemble prediction from OOF predictions\"\"\"\n",
        "        oof_preds = self.oof_predictions[sample_id]\n",
        "        \n",
        "        if method == \"mean\":\n",
        "            return np.mean(oof_preds, axis=0)\n",
        "        elif method == \"max\":\n",
        "            return np.max(oof_preds, axis=0)\n",
        "        elif method == \"median\":\n",
        "            return np.median(oof_preds, axis=0)\n",
        "        else:\n",
        "            return np.mean(oof_preds, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_per_class_misclassifications(self, true_labels_df: pd.DataFrame):\n",
        "    \"\"\"Detailed analysis of misclassifications per class\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for class_idx, class_name in enumerate(LABEL_COLS):\n",
        "        class_analysis = {\n",
        "            'class_name': class_name,\n",
        "            'total_samples': 0,\n",
        "            'positive_samples': 0,\n",
        "            'misclassified_samples': [],\n",
        "            'false_positives': [],\n",
        "            'false_negatives': [],\n",
        "            'confidence_distribution': {'correct': [], 'incorrect': []},\n",
        "            'fold_agreement': []\n",
        "        }\n",
        "        \n",
        "        for sample_id in self.oof_predictions.keys():\n",
        "            if sample_id not in true_labels_df[ID_COL].values:\n",
        "                continue\n",
        "                \n",
        "            true_label = true_labels_df[true_labels_df[ID_COL] == sample_id][class_name].iloc[0]\n",
        "            oof_preds = self.oof_predictions[sample_id]\n",
        "            ensemble_pred = self.create_oof_ensemble(sample_id)\n",
        "            \n",
        "            class_analysis['total_samples'] += 1\n",
        "            \n",
        "            if true_label == 1:  # Positive sample\n",
        "                class_analysis['positive_samples'] += 1\n",
        "                \n",
        "                # Check if misclassified\n",
        "                if ensemble_pred[class_idx] < 0.5:\n",
        "                    class_analysis['false_negatives'].append({\n",
        "                        'sample_id': sample_id,\n",
        "                        'true_label': true_label,\n",
        "                        'prediction': ensemble_pred[class_idx],\n",
        "                        'fold_predictions': [pred[class_idx] for pred in oof_preds],\n",
        "                        'fold_agreement': np.std([pred[class_idx] for pred in oof_preds])\n",
        "                    })\n",
        "            \n",
        "            else:  # Negative sample\n",
        "                if ensemble_pred[class_idx] >= 0.5:\n",
        "                    class_analysis['false_positives'].append({\n",
        "                        'sample_id': sample_id,\n",
        "                        'true_label': true_label,\n",
        "                        'prediction': ensemble_pred[class_idx],\n",
        "                        'fold_predictions': [pred[class_idx] for pred in oof_preds],\n",
        "                        'fold_agreement': np.std([pred[class_idx] for pred in oof_preds])\n",
        "                    })\n",
        "            \n",
        "            # Track confidence distribution\n",
        "            is_correct = (true_label == 1) == (ensemble_pred[class_idx] >= 0.5)\n",
        "            class_analysis['confidence_distribution']['correct' if is_correct else 'incorrect'].append(\n",
        "                ensemble_pred[class_idx]\n",
        "            )\n",
        "            \n",
        "            # Track fold agreement\n",
        "            fold_scores = [pred[class_idx] for pred in oof_preds]\n",
        "            class_analysis['fold_agreement'].append(np.std(fold_scores))\n",
        "        \n",
        "        results[class_name] = class_analysis\n",
        "    \n",
        "    return results\n",
        "\n",
        "def identify_hard_samples(self, true_labels_df: pd.DataFrame):\n",
        "    \"\"\"Identify different types of hard samples\"\"\"\n",
        "    hard_samples = {\n",
        "        'high_confidence_wrong': [],      # Model very confident but wrong\n",
        "        'low_confidence_correct': [],     # Model uncertain but correct\n",
        "        'fold_disagreement': [],          # Folds disagree strongly\n",
        "        'ambiguous_boundary': [],         # Near decision boundary\n",
        "        'rare_class_misclassified': []    # Misclassified rare class samples\n",
        "    }\n",
        "    \n",
        "    for sample_id in self.oof_predictions.keys():\n",
        "        if sample_id not in true_labels_df[ID_COL].values:\n",
        "            continue\n",
        "            \n",
        "        oof_preds = self.oof_predictions[sample_id]\n",
        "        ensemble_pred = self.create_oof_ensemble(sample_id)\n",
        "        true_labels = true_labels_df[true_labels_df[ID_COL] == sample_id][LABEL_COLS].iloc[0].values\n",
        "        \n",
        "        # Calculate fold agreement (lower std = more agreement)\n",
        "        fold_agreement = np.mean([np.std(pred) for pred in oof_preds])\n",
        "        \n",
        "        # Check each class\n",
        "        for class_idx, class_name in enumerate(LABEL_COLS):\n",
        "            true_label = true_labels[class_idx]\n",
        "            pred_score = ensemble_pred[class_idx]\n",
        "            fold_scores = [pred[class_idx] for pred in oof_preds]\n",
        "            \n",
        "            is_correct = (true_label == 1) == (pred_score >= 0.5)\n",
        "            confidence = max(pred_score, 1 - pred_score)\n",
        "            \n",
        "            sample_info = {\n",
        "                'sample_id': sample_id,\n",
        "                'class_name': class_name,\n",
        "                'true_label': true_label,\n",
        "                'prediction': pred_score,\n",
        "                'confidence': confidence,\n",
        "                'fold_scores': fold_scores,\n",
        "                'fold_agreement': np.std(fold_scores)\n",
        "            }\n",
        "            \n",
        "            # Categorize hard samples\n",
        "            if not is_correct and confidence > 0.8:\n",
        "                hard_samples['high_confidence_wrong'].append(sample_info)\n",
        "            elif is_correct and confidence < 0.3:\n",
        "                hard_samples['low_confidence_correct'].append(sample_info)\n",
        "            elif fold_agreement > 0.3:  # High disagreement between folds\n",
        "                hard_samples['fold_disagreement'].append(sample_info)\n",
        "            elif 0.4 <= pred_score <= 0.6:  # Near decision boundary\n",
        "                hard_samples['ambiguous_boundary'].append(sample_info)\n",
        "    \n",
        "    return hard_samples\n",
        "\n",
        "def analyze_fold_agreement(self):\n",
        "    \"\"\"Analyze how much the 4 folds agree on predictions\"\"\"\n",
        "    agreement_stats = {\n",
        "        'overall_agreement': [],\n",
        "        'per_class_agreement': {class_name: [] for class_name in LABEL_COLS},\n",
        "        'agreement_vs_confidence': [],\n",
        "        'disagreement_cases': []\n",
        "    }\n",
        "    \n",
        "    for sample_id in self.oof_predictions.keys():\n",
        "        oof_preds = self.oof_predictions[sample_id]\n",
        "        \n",
        "        # Calculate agreement for each class\n",
        "        for class_idx, class_name in enumerate(LABEL_COLS):\n",
        "            fold_scores = [pred[class_idx] for pred in oof_preds]\n",
        "            agreement = 1 - np.std(fold_scores)  # Higher = more agreement\n",
        "            confidence = np.mean(fold_scores)\n",
        "            \n",
        "            agreement_stats['per_class_agreement'][class_name].append(agreement)\n",
        "            agreement_stats['agreement_vs_confidence'].append({\n",
        "                'agreement': agreement,\n",
        "                'confidence': confidence,\n",
        "                'sample_id': sample_id,\n",
        "                'class_name': class_name\n",
        "            })\n",
        "            \n",
        "            # Track high disagreement cases\n",
        "            if agreement < 0.5:  # Low agreement threshold\n",
        "                agreement_stats['disagreement_cases'].append({\n",
        "                    'sample_id': sample_id,\n",
        "                    'class_name': class_name,\n",
        "                    'fold_scores': fold_scores,\n",
        "                    'agreement': agreement\n",
        "                })\n",
        "    \n",
        "    return agreement_stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_visualizations(self, per_class_analysis, hard_samples, fold_agreement):\n",
        "    \"\"\"Create comprehensive visualizations\"\"\"\n",
        "    \n",
        "    # 1. Per-class error summary\n",
        "    self._plot_per_class_error_summary(per_class_analysis)\n",
        "    \n",
        "    # 2. Confidence distribution plots\n",
        "    self._plot_confidence_distributions(per_class_analysis)\n",
        "    \n",
        "    # 3. Fold agreement analysis\n",
        "    self._plot_fold_agreement_analysis(fold_agreement)\n",
        "    \n",
        "    # 4. Hard sample analysis\n",
        "    self._plot_hard_sample_analysis(hard_samples)\n",
        "    \n",
        "    # 5. ROC curves per class\n",
        "    self._plot_roc_curves_per_class(per_class_analysis)\n",
        "\n",
        "def _plot_per_class_error_summary(self, per_class_analysis):\n",
        "    \"\"\"Plot per-class error summary\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Error counts\n",
        "    class_names = list(per_class_analysis.keys())\n",
        "    false_negatives = [len(per_class_analysis[cls]['false_negatives']) for cls in class_names]\n",
        "    false_positives = [len(per_class_analysis[cls]['false_positives']) for cls in class_names]\n",
        "    total_errors = [fn + fp for fn, fp in zip(false_negatives, false_positives)]\n",
        "    \n",
        "    # Plot 1: Total errors per class\n",
        "    axes[0, 0].bar(range(len(class_names)), total_errors)\n",
        "    axes[0, 0].set_title('Total Misclassifications per Class')\n",
        "    axes[0, 0].set_ylabel('Count')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 2: False Negatives vs False Positives\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.35\n",
        "    axes[0, 1].bar(x - width/2, false_negatives, width, label='False Negatives', alpha=0.8)\n",
        "    axes[0, 1].bar(x + width/2, false_positives, width, label='False Positives', alpha=0.8)\n",
        "    axes[0, 1].set_title('False Negatives vs False Positives')\n",
        "    axes[0, 1].set_ylabel('Count')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 3: Error rate per class\n",
        "    error_rates = []\n",
        "    for cls in class_names:\n",
        "        total_samples = per_class_analysis[cls]['total_samples']\n",
        "        errors = len(per_class_analysis[cls]['false_negatives']) + len(per_class_analysis[cls]['false_positives'])\n",
        "        error_rate = errors / total_samples if total_samples > 0 else 0\n",
        "        error_rates.append(error_rate)\n",
        "    \n",
        "    axes[1, 0].bar(range(len(class_names)), error_rates)\n",
        "    axes[1, 0].set_title('Error Rate per Class')\n",
        "    axes[1, 0].set_ylabel('Error Rate')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 4: Positive sample distribution\n",
        "    positive_counts = [per_class_analysis[cls]['positive_samples'] for cls in class_names]\n",
        "    axes[1, 1].bar(range(len(class_names)), positive_counts)\n",
        "    axes[1, 1].set_title('Positive Samples per Class')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_confidence_distributions(self, per_class_analysis):\n",
        "    \"\"\"Plot confidence distributions for correct vs incorrect predictions\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    # Select 4 classes to plot\n",
        "    selected_classes = list(per_class_analysis.keys())[:4]\n",
        "    \n",
        "    for i, class_name in enumerate(selected_classes):\n",
        "        if i >= 4:\n",
        "            break\n",
        "            \n",
        "        correct_conf = per_class_analysis[class_name]['confidence_distribution']['correct']\n",
        "        incorrect_conf = per_class_analysis[class_name]['confidence_distribution']['incorrect']\n",
        "        \n",
        "        axes[i].hist(correct_conf, bins=20, alpha=0.7, label='Correct', density=True)\n",
        "        axes[i].hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', density=True)\n",
        "        axes[i].set_title(f'{class_name}\\\\nConfidence Distribution')\n",
        "        axes[i].set_xlabel('Prediction Confidence')\n",
        "        axes[i].set_ylabel('Density')\n",
        "        axes[i].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_fold_agreement_analysis(self, fold_agreement):\n",
        "    \"\"\"Plot fold agreement analysis\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot 1: Agreement distribution\n",
        "    all_agreements = []\n",
        "    for class_name in LABEL_COLS:\n",
        "        all_agreements.extend(fold_agreement['per_class_agreement'][class_name])\n",
        "    \n",
        "    axes[0].hist(all_agreements, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_title('Fold Agreement Distribution')\n",
        "    axes[0].set_xlabel('Agreement Score (1 - std)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].axvline(np.mean(all_agreements), color='red', linestyle='--', \n",
        "                    label=f'Mean: {np.mean(all_agreements):.3f}')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Plot 2: Agreement vs Confidence\n",
        "    agreement_data = fold_agreement['agreement_vs_confidence']\n",
        "    agreements = [item['agreement'] for item in agreement_data]\n",
        "    confidences = [item['confidence'] for item in agreement_data]\n",
        "    \n",
        "    scatter = axes[1].scatter(confidences, agreements, alpha=0.6, s=20)\n",
        "    axes[1].set_title('Fold Agreement vs Prediction Confidence')\n",
        "    axes[1].set_xlabel('Prediction Confidence')\n",
        "    axes[1].set_ylabel('Fold Agreement')\n",
        "    \n",
        "    # Add correlation coefficient\n",
        "    corr = np.corrcoef(confidences, agreements)[0, 1]\n",
        "    axes[1].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
        "                transform=axes[1].transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_hard_sample_analysis(self, hard_samples):\n",
        "    \"\"\"Plot hard sample analysis\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Count hard samples by type\n",
        "    hard_sample_counts = {\n",
        "        'High Confidence Wrong': len(hard_samples['high_confidence_wrong']),\n",
        "        'Low Confidence Correct': len(hard_samples['low_confidence_correct']),\n",
        "        'Fold Disagreement': len(hard_samples['fold_disagreement']),\n",
        "        'Ambiguous Boundary': len(hard_samples['ambiguous_boundary'])\n",
        "    }\n",
        "    \n",
        "    # Plot 1: Hard sample counts\n",
        "    axes[0, 0].bar(hard_sample_counts.keys(), hard_sample_counts.values())\n",
        "    axes[0, 0].set_title('Hard Sample Counts by Type')\n",
        "    axes[0, 0].set_ylabel('Count')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 2: Hard samples by class\n",
        "    class_hard_counts = {}\n",
        "    for hard_type, samples in hard_samples.items():\n",
        "        for sample in samples:\n",
        "            class_name = sample['class_name']\n",
        "            if class_name not in class_hard_counts:\n",
        "                class_hard_counts[class_name] = 0\n",
        "            class_hard_counts[class_name] += 1\n",
        "    \n",
        "    if class_hard_counts:\n",
        "        axes[0, 1].bar(class_hard_counts.keys(), class_hard_counts.values())\n",
        "        axes[0, 1].set_title('Hard Samples by Class')\n",
        "        axes[0, 1].set_ylabel('Count')\n",
        "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 3: Confidence distribution for hard samples\n",
        "    all_confidences = []\n",
        "    all_types = []\n",
        "    for hard_type, samples in hard_samples.items():\n",
        "        for sample in samples:\n",
        "            all_confidences.append(sample['confidence'])\n",
        "            all_types.append(hard_type)\n",
        "    \n",
        "    if all_confidences:\n",
        "        for hard_type in set(all_types):\n",
        "            type_confidences = [conf for conf, t in zip(all_confidences, all_types) if t == hard_type]\n",
        "            axes[1, 0].hist(type_confidences, alpha=0.6, label=hard_type, bins=15)\n",
        "        axes[1, 0].set_title('Confidence Distribution by Hard Sample Type')\n",
        "        axes[1, 0].set_xlabel('Confidence')\n",
        "        axes[1, 0].set_ylabel('Frequency')\n",
        "        axes[1, 0].legend()\n",
        "    \n",
        "    # Plot 4: Fold agreement for hard samples\n",
        "    fold_agreements = []\n",
        "    for hard_type, samples in hard_samples.items():\n",
        "        for sample in samples:\n",
        "            fold_agreements.append(sample['fold_agreement'])\n",
        "    \n",
        "    if fold_agreements:\n",
        "        axes[1, 1].hist(fold_agreements, bins=20, alpha=0.7, edgecolor='black')\n",
        "        axes[1, 1].set_title('Fold Agreement Distribution for Hard Samples')\n",
        "        axes[1, 1].set_xlabel('Fold Agreement (std)')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_roc_curves_per_class(self, per_class_analysis):\n",
        "    \"\"\"Plot ROC curves for each class\"\"\"\n",
        "    fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, class_name in enumerate(LABEL_COLS):\n",
        "        if i >= len(axes):\n",
        "            break\n",
        "            \n",
        "        # Collect true labels and predictions for this class\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        \n",
        "        for sample_id in self.oof_predictions.keys():\n",
        "            if sample_id in self.true_labels:\n",
        "                true_label = self.true_labels[sample_id][i]\n",
        "                ensemble_pred = self.create_oof_ensemble(sample_id)\n",
        "                pred_score = ensemble_pred[i]\n",
        "                \n",
        "                y_true.append(true_label)\n",
        "                y_pred.append(pred_score)\n",
        "        \n",
        "        if len(set(y_true)) > 1:  # Only plot if we have both classes\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "            auc = roc_auc_score(y_true, y_pred)\n",
        "            \n",
        "            axes[i].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
        "            axes[i].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "            axes[i].set_title(f'{class_name}\\\\nAUC: {auc:.3f}')\n",
        "            axes[i].set_xlabel('False Positive Rate')\n",
        "            axes[i].set_ylabel('True Positive Rate')\n",
        "            axes[i].legend()\n",
        "        else:\n",
        "            axes[i].text(0.5, 0.5, 'Insufficient data', \n",
        "                        ha='center', va='center', transform=axes[i].transAxes)\n",
        "            axes[i].set_title(class_name)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(LABEL_COLS), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize analyzer\n",
        "analyzer = CrossFoldAnalyzer(EXPERIMENT_DIR, config)\n",
        "\n",
        "# Load fold assignments\n",
        "train_csv_path = config_dict['paths']['train_csv']\n",
        "analyzer.load_fold_assignments(train_csv_path)\n",
        "\n",
        "# Load models\n",
        "analyzer.load_fold_models()\n",
        "\n",
        "# Get sample IDs for analysis (use a subset for testing)\n",
        "all_sample_ids = list(analyzer.fold_assignments.keys())\n",
        "print(f\"Total samples available: {len(all_sample_ids)}\")\n",
        "\n",
        "# For testing, use a subset (remove this for full analysis)\n",
        "test_sample_ids = all_sample_ids[:100]  # First 100 samples for testing\n",
        "print(f\"Using {len(test_sample_ids)} samples for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect OOF predictions\n",
        "analyzer.collect_oof_predictions(test_sample_ids)\n",
        "\n",
        "print(f\"Collected OOF predictions for {len(analyzer.oof_predictions)} samples\")\n",
        "print(f\"Average predictions per sample: {np.mean([len(preds) for preds in analyzer.oof_predictions.values()]):.1f}\")\n",
        "\n",
        "# Load true labels for analysis\n",
        "true_labels_df = pd.read_csv(train_csv_path)\n",
        "print(f\"Loaded true labels for {len(true_labels_df)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive analysis\n",
        "print(\"🔍 Starting Cross-Fold Analysis...\")\n",
        "\n",
        "# 1. Per-class misclassification analysis\n",
        "print(\"📊 Analyzing per-class misclassifications...\")\n",
        "per_class_analysis = analyzer.analyze_per_class_misclassifications(true_labels_df)\n",
        "\n",
        "# 2. Hard sample identification\n",
        "print(\"🎯 Identifying hard samples...\")\n",
        "hard_samples = analyzer.identify_hard_samples(true_labels_df)\n",
        "\n",
        "# 3. Fold agreement analysis\n",
        "print(\"🤝 Analyzing fold agreement...\")\n",
        "fold_agreement = analyzer.analyze_fold_agreement()\n",
        "\n",
        "print(\"✅ Analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualizations\n",
        "print(\"📈 Creating visualizations...\")\n",
        "analyzer.create_visualizations(per_class_analysis, hard_samples, fold_agreement)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print comprehensive results summary\n",
        "print(\"=\" * 80)\n",
        "print(\"📋 CROSS-FOLD ANALYSIS RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Overall statistics\n",
        "total_samples = len(analyzer.oof_predictions)\n",
        "print(f\"\\\\n📊 Overall Statistics:\")\n",
        "print(f\"  • Total samples analyzed: {total_samples}\")\n",
        "print(f\"  • Average OOF predictions per sample: {np.mean([len(preds) for preds in analyzer.oof_predictions.values()]):.1f}\")\n",
        "\n",
        "# Per-class summary\n",
        "print(f\"\\\\n🎯 Per-Class Misclassification Summary:\")\n",
        "for class_name, analysis in per_class_analysis.items():\n",
        "    fn_count = len(analysis['false_negatives'])\n",
        "    fp_count = len(analysis['false_positives'])\n",
        "    total_errors = fn_count + fp_count\n",
        "    error_rate = total_errors / analysis['total_samples'] if analysis['total_samples'] > 0 else 0\n",
        "    \n",
        "    print(f\"  • {class_name}:\")\n",
        "    print(f\"    - Total samples: {analysis['total_samples']}\")\n",
        "    print(f\"    - Positive samples: {analysis['positive_samples']}\")\n",
        "    print(f\"    - False Negatives: {fn_count}\")\n",
        "    print(f\"    - False Positives: {fp_count}\")\n",
        "    print(f\"    - Error Rate: {error_rate:.3f}\")\n",
        "\n",
        "# Hard samples summary\n",
        "print(f\"\\\\n🎯 Hard Sample Summary:\")\n",
        "for hard_type, samples in hard_samples.items():\n",
        "    print(f\"  • {hard_type}: {len(samples)} samples\")\n",
        "\n",
        "# Fold agreement summary\n",
        "print(f\"\\\\n🤝 Fold Agreement Summary:\")\n",
        "all_agreements = []\n",
        "for class_name in LABEL_COLS:\n",
        "    all_agreements.extend(fold_agreement['per_class_agreement'][class_name])\n",
        "\n",
        "if all_agreements:\n",
        "    print(f\"  • Mean agreement: {np.mean(all_agreements):.3f}\")\n",
        "    print(f\"  • Std agreement: {np.std(all_agreements):.3f}\")\n",
        "    print(f\"  • Min agreement: {np.min(all_agreements):.3f}\")\n",
        "    print(f\"  • Max agreement: {np.max(all_agreements):.3f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 80)\n",
        "print(\"✅ Analysis Complete! Check visualizations above for detailed insights.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
