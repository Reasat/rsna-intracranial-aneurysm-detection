{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA 2025 Intracranial Aneurysm Detection - Two-Step Inference\n",
    "\n",
    "This notebook performs two-step inference using trained 2.5D EfficientNet hybrid models.\n",
    "\n",
    "## Two-Step Approach\n",
    "1. **Step 1**: 14-class model predicts all 13 anatomical locations + \"Aneurysm Present\"\n",
    "2. **Step 2**: Binary model predicts refined \"Aneurysm Present\" \n",
    "3. **Final Output**: Use binary model's \"Aneurysm Present\" to overwrite 14th class\n",
    "\n",
    "## Model Details\n",
    "- Architecture: tf_efficientnet_b0\n",
    "- Training: 5-fold cross-validation\n",
    "- Input: 2.5D windows (5-slice)\n",
    "- Dual-stream: Full image + ROI processing\n",
    "- Models: 14-class (2025-09-11-20-34-47) + Binary (2025-09-15-05-27-19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import shutil\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Import normalization functions from dedicated module\n",
    "from normalization import normalize_dicom_series, apply_rescale_intercept_slope\n",
    "\n",
    "# Import utility functions\n",
    "from utils import (\n",
    "    take_window, coords_to_px, crop_and_resize_hwc, make_bbox_px, \n",
    "    valid_coords, load_cached_volume, window_to_full_and_roi\n",
    ")\n",
    "\n",
    "# Import data processing functions\n",
    "from data_processing import sort_dicom_slices\n",
    "\n",
    "# Import model classes and creation functions\n",
    "from model import HybridAneurysmModel, BinaryAneurysmModel, create_model, create_binary_model\n",
    "\n",
    "# Kaggle server\n",
    "import kaggle_evaluation.rsna_inference_server\n",
    "\n",
    "# ========= Competition schema =========\n",
    "ID_COL = 'SeriesInstanceUID'\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "\n",
    "# ========= Inference config =========\n",
    "IMG_SIZE = 224\n",
    "OFFSETS = (-2, -1, 0, 1, 2)   # window length 5\n",
    "IN_CHANS = len(OFFSETS)\n",
    "BATCH_SIZE = 128\n",
    "AGGREGATE = \"max\"  # max/mean/topk_mean\n",
    "USE_ROI = False     # coords not available on test â†’ use same stream for full+roi\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model weights location - update this path to match your uploaded dataset\n",
    "CANDIDATE_MODEL_DIRS = [\n",
    "    \"/kaggle/input/2025-09-11-20-34-47\",        # 14-class models\n",
    "    \"/kaggle/input/2025-09-15-05-27-19\",        # binary models\n",
    "    \"/kaggle/working\",                           # runtime dir\n",
    "    \".\",                                         # current dir\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Model configuration =========\n",
    "# Model classes are imported from model.py\n",
    "# HybridAneurysmModel: 14-class classification model (first step)\n",
    "# BinaryAneurysmModel: Binary classification model (second step)\n",
    "\n",
    "class SimpleConfig:\n",
    "    \"\"\"Simple config class for model initialization\"\"\"\n",
    "    def __init__(self, architecture: str, in_channels: int, num_classes: int):\n",
    "        self.architecture = architecture\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "# Create config objects for both model types\n",
    "config_14class = SimpleConfig(\"tf_efficientnet_b0\", IN_CHANS, len(LABEL_COLS))\n",
    "config_binary = SimpleConfig(\"tf_efficientnet_b0\", IN_CHANS, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Model loading and discovery =========\n",
    "_ckpt_cache_14class = None\n",
    "_ckpt_cache_binary = None\n",
    "_models_14class = None\n",
    "_models_binary = None\n",
    "\n",
    "def discover_checkpoints_14class() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Discover 14-class model checkpoints\"\"\"\n",
    "    found: List[Tuple[str, str]] = []\n",
    "    for base in CANDIDATE_MODEL_DIRS:\n",
    "        if not os.path.isdir(base):\n",
    "            continue\n",
    "        for root, _, files in os.walk(base):\n",
    "            for f in files:\n",
    "                if f.endswith('.pth') and 'tf_efficientnet_b0' in f and 'fold' in f and ('best' in f or 'final' in f):\n",
    "                    # Check if it's from the 14-class training (models/2025-09-11-20-34-47)\n",
    "                    if '2025-09-11-20-34-47' in root or '14class' in f.lower():\n",
    "                        arch = 'tf_efficientnet_b0'\n",
    "                        found.append((arch, os.path.join(root, f)))\n",
    "    found.sort(key=lambda x: x[1])\n",
    "    return found\n",
    "\n",
    "def discover_checkpoints_binary() -> List[Tuple[str, str]]:\n",
    "    \"\"\"Discover binary model checkpoints\"\"\"\n",
    "    found: List[Tuple[str, str]] = []\n",
    "    for base in CANDIDATE_MODEL_DIRS:\n",
    "        if not os.path.isdir(base):\n",
    "            continue\n",
    "        for root, _, files in os.walk(base):\n",
    "            for f in files:\n",
    "                if f.endswith('.pth') and 'tf_efficientnet_b0' in f and 'fold' in f and ('best' in f or 'final' in f):\n",
    "                    # Check if it's from the binary training (models/2025-09-15-05-27-19)\n",
    "                    if '2025-09-15-05-27-19' in root or 'binary' in f.lower():\n",
    "                        arch = 'tf_efficientnet_b0'\n",
    "                        found.append((arch, os.path.join(root, f)))\n",
    "    found.sort(key=lambda x: x[1])\n",
    "    return found\n",
    "\n",
    "def load_hybrid_model(arch_name: str, weight_path: str, num_classes: int) -> nn.Module:\n",
    "    \"\"\"Load 14-class model using model.py creation function\"\"\"\n",
    "    model = create_model(config_14class)\n",
    "    state = torch.load(weight_path, map_location=DEVICE)\n",
    "    \n",
    "    # Handle different state dict formats\n",
    "    if isinstance(state, dict) and 'model_state_dict' in state:\n",
    "        state = state['model_state_dict']\n",
    "    elif isinstance(state, dict) and any(k.startswith('module.') for k in state.keys()):\n",
    "        state = {k.replace('module.', '', 1): v for k, v in state.items()}\n",
    "    \n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval().to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def load_binary_model(arch_name: str, weight_path: str, num_classes: int) -> nn.Module:\n",
    "    \"\"\"Load binary model using model.py creation function\"\"\"\n",
    "    model = create_binary_model(config_binary)\n",
    "    state = torch.load(weight_path, map_location=DEVICE)\n",
    "    \n",
    "    # Handle different state dict formats\n",
    "    if isinstance(state, dict) and 'model_state_dict' in state:\n",
    "        state = state['model_state_dict']\n",
    "    elif isinstance(state, dict) and any(k.startswith('module.') for k in state.keys()):\n",
    "        state = {k.replace('module.', '', 1): v for k, v in state.items()}\n",
    "    \n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval().to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def get_models_14class() -> List[Tuple[str, nn.Module]]:\n",
    "    \"\"\"Get 14-class models\"\"\"\n",
    "    global _ckpt_cache_14class, _models_14class\n",
    "    if _models_14class is not None:\n",
    "        return _models_14class\n",
    "    _ckpt_cache_14class = discover_checkpoints_14class()\n",
    "    if not _ckpt_cache_14class:\n",
    "        raise FileNotFoundError('No 14-class model checkpoints found. Make sure model dataset is attached.')\n",
    "    mods: List[Tuple[str, nn.Module]] = []\n",
    "    for arch, path in _ckpt_cache_14class:\n",
    "        try:\n",
    "            m = load_hybrid_model(arch, path, len(LABEL_COLS))\n",
    "            mods.append((arch, m))\n",
    "            print(f\"Loaded 14-class model: {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load 14-class model {path}: {e}\")\n",
    "            continue\n",
    "    if not mods:\n",
    "        raise RuntimeError('Failed to load any 14-class checkpoints from discovered files.')\n",
    "    _models_14class = mods\n",
    "    print(f\"Loaded {len(_models_14class)} 14-class models total\")\n",
    "    return _models_14class\n",
    "\n",
    "def get_models_binary() -> List[Tuple[str, nn.Module]]:\n",
    "    \"\"\"Get binary models\"\"\"\n",
    "    global _ckpt_cache_binary, _models_binary\n",
    "    if _models_binary is not None:\n",
    "        return _models_binary\n",
    "    _ckpt_cache_binary = discover_checkpoints_binary()\n",
    "    if not _ckpt_cache_binary:\n",
    "        raise FileNotFoundError('No binary model checkpoints found. Make sure model dataset is attached.')\n",
    "    mods: List[Tuple[str, nn.Module]] = []\n",
    "    for arch, path in _ckpt_cache_binary:\n",
    "        try:\n",
    "            m = load_binary_model(arch, path, 1)  # Binary model has 1 output\n",
    "            mods.append((arch, m))\n",
    "            print(f\"Loaded binary model: {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load binary model {path}: {e}\")\n",
    "            continue\n",
    "    if not mods:\n",
    "        raise RuntimeError('Failed to load any binary checkpoints from discovered files.')\n",
    "    _models_binary = mods\n",
    "    print(f\"Loaded {len(_models_binary)} binary models total\")\n",
    "    return _models_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Two-step inference pipeline =========\n",
    "@torch.no_grad()\n",
    "def predict_series_probs_two_step(dicoms) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Two-step inference pipeline:\n",
    "    1. First step: 14-class model predicts all 13 anatomical locations + \"Aneurysm Present\"\n",
    "    2. Second step: Binary model predicts refined \"Aneurysm Present\" \n",
    "    3. Final output: Use binary model's \"Aneurysm Present\" to overwrite 14th class\n",
    "    \"\"\"\n",
    "    # Get both model types\n",
    "    models_14class = get_models_14class()\n",
    "    models_binary = get_models_binary()\n",
    "    \n",
    "    # Build normalized volume [N,H,W] uint8 (matching training data format)\n",
    "    vol = normalize_dicom_series(dicoms, target_size=IMG_SIZE, apply_rescale=True)\n",
    "    N = vol.shape[0]\n",
    "    # Prepare coords zeros on test\n",
    "    coords = np.zeros((N, 2), dtype=np.float32)\n",
    "\n",
    "    # Step 1: 14-class prediction\n",
    "    print(\"Step 1: Running 14-class models...\")\n",
    "    all_14class_probs = []\n",
    "    for _, model in models_14class:\n",
    "        batch_full, batch_roi, batch_coords = [], [], []\n",
    "        probs_accum = []\n",
    "        for c in range(N):\n",
    "            win = take_window(vol, c, OFFSETS)   # [C,H,W] float32\n",
    "            win_chw = np.transpose(win, (0, 1, 2))           # still [C,H,W]\n",
    "            full_chw, roi_chw = window_to_full_and_roi(win_chw, coords[c], IMG_SIZE)\n",
    "            batch_full.append(full_chw)\n",
    "            batch_roi.append(roi_chw)\n",
    "            batch_coords.append(coords[c])\n",
    "            # flush by batch\n",
    "            if len(batch_full) == BATCH_SIZE or c == N - 1:\n",
    "                xb_full = torch.from_numpy(np.stack(batch_full).astype(np.float32)).to(DEVICE)\n",
    "                xb_roi  = torch.from_numpy(np.stack(batch_roi).astype(np.float32)).to(DEVICE)\n",
    "                cb      = torch.from_numpy(np.stack(batch_coords).astype(np.float32)).to(DEVICE)\n",
    "                logits = model(xb_full, xb_roi, cb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                probs_accum.append(probs)\n",
    "                batch_full.clear(); batch_roi.clear(); batch_coords.clear()\n",
    "        probs_all = np.concatenate(probs_accum, axis=0) if probs_accum else np.zeros((1, len(LABEL_COLS)), dtype=np.float32)\n",
    "        if AGGREGATE == 'max':\n",
    "            series_prob = probs_all.max(axis=0)\n",
    "        elif AGGREGATE == 'mean':\n",
    "            series_prob = probs_all.mean(axis=0)\n",
    "        else:  # topk_mean\n",
    "            k = max(1, N // 5)\n",
    "            series_prob = np.sort(probs_all, axis=0)[-k:].mean(axis=0)\n",
    "        all_14class_probs.append(series_prob)\n",
    "        # free memory between models\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Ensemble 14-class predictions\n",
    "    probs_14class = np.mean(np.stack(all_14class_probs, axis=0), axis=0)\n",
    "    \n",
    "    # Step 2: Binary prediction\n",
    "    print(\"Step 2: Running binary models...\")\n",
    "    all_binary_probs = []\n",
    "    for _, model in models_binary:\n",
    "        batch_full, batch_roi, batch_coords = [], [], []\n",
    "        probs_accum = []\n",
    "        for c in range(N):\n",
    "            win = take_window(vol, c, OFFSETS)   # [C,H,W] float32\n",
    "            win_chw = np.transpose(win, (0, 1, 2))           # still [C,H,W]\n",
    "            full_chw, roi_chw = window_to_full_and_roi(win_chw, coords[c], IMG_SIZE)\n",
    "            batch_full.append(full_chw)\n",
    "            batch_roi.append(roi_chw)\n",
    "            batch_coords.append(coords[c])\n",
    "            # flush by batch\n",
    "            if len(batch_full) == BATCH_SIZE or c == N - 1:\n",
    "                xb_full = torch.from_numpy(np.stack(batch_full).astype(np.float32)).to(DEVICE)\n",
    "                xb_roi  = torch.from_numpy(np.stack(batch_roi).astype(np.float32)).to(DEVICE)\n",
    "                cb      = torch.from_numpy(np.stack(batch_coords).astype(np.float32)).to(DEVICE)\n",
    "                logits = model(xb_full, xb_roi, cb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                probs_accum.append(probs)\n",
    "                batch_full.clear(); batch_roi.clear(); batch_coords.clear()\n",
    "        probs_all = np.concatenate(probs_accum, axis=0) if probs_accum else np.zeros((1, 1), dtype=np.float32)\n",
    "        if AGGREGATE == 'max':\n",
    "            series_prob = probs_all.max(axis=0)\n",
    "        elif AGGREGATE == 'mean':\n",
    "            series_prob = probs_all.mean(axis=0)\n",
    "        else:  # topk_mean\n",
    "            k = max(1, N // 5)\n",
    "            series_prob = np.sort(probs_all, axis=0)[-k:].mean(axis=0)\n",
    "        all_binary_probs.append(series_prob)\n",
    "        # free memory between models\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Ensemble binary predictions\n",
    "    probs_binary = np.mean(np.stack(all_binary_probs, axis=0), axis=0)\n",
    "    \n",
    "    # Step 3: Combine results - use binary model's \"Aneurysm Present\" to overwrite 14th class\n",
    "    final_probs = probs_14class.copy()\n",
    "    final_probs[-1] = probs_binary[0]  # Overwrite \"Aneurysm Present\" with binary prediction\n",
    "    \n",
    "    print(f\"14-class 'Aneurysm Present': {probs_14class[-1]:.4f}\")\n",
    "    print(f\"Binary 'Aneurysm Present': {probs_binary[0]:.4f}\")\n",
    "    print(f\"Final 'Aneurysm Present': {final_probs[-1]:.4f}\")\n",
    "    \n",
    "    return final_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Kaggle-required predict function =========\n",
    "def predict(series_path: str) -> pl.DataFrame | pd.DataFrame:\n",
    "    series_id = os.path.basename(series_path)\n",
    "\n",
    "    # Collect all DICOM files\n",
    "    filepaths = []\n",
    "    for root, _, files in os.walk(series_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.dcm'):\n",
    "                filepaths.append(os.path.join(root, f))\n",
    "    \n",
    "    if not filepaths:\n",
    "        # Return zeros if no DICOMs found\n",
    "        zeros = [[series_id] + [0.0] * len(LABEL_COLS)]\n",
    "        predictions = pl.DataFrame(data=zeros, schema=[ID_COL, *LABEL_COLS], orient='row')\n",
    "        return predictions.drop(ID_COL)\n",
    "    \n",
    "    # Sort DICOMs and perform two-step inference\n",
    "    dicoms = sort_dicom_slices(filepaths)\n",
    "    probs = predict_series_probs_two_step(dicoms)\n",
    "\n",
    "    # Build output (one row)\n",
    "    data = [[series_id] + probs.tolist()]\n",
    "    predictions = pl.DataFrame(data=data, schema=[ID_COL, *LABEL_COLS], orient='row')\n",
    "\n",
    "    # Required cleanup to avoid disk pressure\n",
    "    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n",
    "\n",
    "    # Server expects features only (without ID_COL)\n",
    "    return predictions.drop(ID_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading and basic functionality\n",
    "print(\"Testing two-step model discovery...\")\n",
    "try:\n",
    "    checkpoints_14class = discover_checkpoints_14class()\n",
    "    print(f\"Found {len(checkpoints_14class)} 14-class model checkpoints:\")\n",
    "    for arch, path in checkpoints_14class:\n",
    "        print(f\"  - {arch}: {path}\")\n",
    "    \n",
    "    checkpoints_binary = discover_checkpoints_binary()\n",
    "    print(f\"Found {len(checkpoints_binary)} binary model checkpoints:\")\n",
    "    for arch, path in checkpoints_binary:\n",
    "        print(f\"  - {arch}: {path}\")\n",
    "        \n",
    "    if len(checkpoints_14class) == 0:\n",
    "        print(\"WARNING: No 14-class models found!\")\n",
    "    if len(checkpoints_binary) == 0:\n",
    "        print(\"WARNING: No binary models found!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error discovering models: {e}\")\n",
    "\n",
    "# Test if we're in competition mode\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Running in test mode - not competition submission\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Start RSNA inference server =========\n",
    "inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # Competition mode - serve predictions\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Test mode - run local gateway\n",
    "    inference_server.run_local_gateway()\n",
    "    if os.path.exists('/kaggle/working/submission.parquet'):\n",
    "        print(\"Submission file created successfully\")\n",
    "        submission_df = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "        print(f\"Submission shape: {submission_df.shape}\")\n",
    "        print(submission_df.head())\n",
    "    else:\n",
    "        print(\"No submission file generated\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
