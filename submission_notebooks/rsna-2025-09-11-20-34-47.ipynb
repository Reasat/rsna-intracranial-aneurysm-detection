{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA 2025 Intracranial Aneurysm Detection - Inference\n",
    "\n",
    "This notebook performs inference using the trained 2.5D EfficientNet hybrid model.\n",
    "\n",
    "## Model Details\n",
    "- Architecture: tf_efficientnet_b0\n",
    "- Training: 5-fold cross-validation\n",
    "- Input: 2.5D windows (5-slice)\n",
    "- Dual-stream: Full image + ROI processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import shutil\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Kaggle server\n",
    "import kaggle_evaluation.rsna_inference_server\n",
    "\n",
    "# ========= Competition schema =========\n",
    "ID_COL = 'SeriesInstanceUID'\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "\n",
    "# ========= Inference config =========\n",
    "IMG_SIZE = 224\n",
    "OFFSETS = (-2, -1, 0, 1, 2)   # window length 5\n",
    "IN_CHANS = len(OFFSETS)\n",
    "BATCH_SIZE = 16\n",
    "AGGREGATE = \"max\"  # max/mean/topk_mean\n",
    "USE_ROI = False     # coords not available on test \u2192 use same stream for full+roi\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model weights location - update this path to match your uploaded dataset\n",
    "CANDIDATE_MODEL_DIRS = [\n",
    "    \"/kaggle/input/2025-09-11-20-34-47\",\n",
    "    \"/kaggle/working\",                           # runtime dir\n",
    "    \".\",                                         # current dir\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Model definition (Hybrid full + ROI + coords) =========\n",
    "class HybridAneurysmModel(nn.Module):\n",
    "    def __init__(self, base_model_name: str, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(base_model_name, in_chans=IN_CHANS, num_classes=0, pretrained=False)\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        self.coord_fc = nn.Sequential(nn.Linear(2, 32), nn.ReLU(), nn.Linear(32, 64))\n",
    "        self.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(self.feature_dim * 2 + 64, num_classes))\n",
    "\n",
    "    def forward(self, x_full: torch.Tensor, x_roi: torch.Tensor, coords: torch.Tensor) -> torch.Tensor:\n",
    "        f_full = self.backbone(x_full)\n",
    "        f_roi  = self.backbone(x_roi)\n",
    "        f_coord = self.coord_fc(coords.float())\n",
    "        return self.fc(torch.cat([f_full, f_roi, f_coord], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Helper functions =========\n",
    "def sort_dicom_slices(filepaths: List[str]):\n",
    "    dicoms = [pydicom.dcmread(fp, force=True) for fp in filepaths]\n",
    "    try:\n",
    "        dicoms.sort(key=lambda d: float(d.ImagePositionPatient[2]))\n",
    "    except Exception:\n",
    "        dicoms.sort(key=lambda d: int(getattr(d, 'InstanceNumber', 0)))\n",
    "    return dicoms\n",
    "\n",
    "def series_to_tensor_chw(dicoms) -> np.ndarray:\n",
    "    # Resize all to IMG_SIZE and apply modality-specific normalization (matching training)\n",
    "    resized = []\n",
    "    for d in dicoms:\n",
    "        arr = d.pixel_array\n",
    "        if arr is None or arr.size == 0:\n",
    "            continue\n",
    "        arr = arr.astype(np.float32)\n",
    "        \n",
    "        # Apply RescaleSlope and RescaleIntercept\n",
    "        slope = getattr(d, 'RescaleSlope', 1)\n",
    "        intercept = getattr(d, 'RescaleIntercept', 0)\n",
    "        if slope != 1 or intercept != 0:\n",
    "            arr = arr * float(slope) + float(intercept)\n",
    "        \n",
    "        # Apply modality-specific normalization (matching training data processing)\n",
    "        modality = getattr(d, 'Modality', 'MR')\n",
    "        if modality == 'CT':\n",
    "            # CT: Fixed range normalization [0, 500] \u2192 [0, 255]\n",
    "            arr = np.clip(arr, 0, 500)\n",
    "            arr = (arr - 0) / (500 - 0)\n",
    "            arr = (arr * 255).astype(np.uint8)\n",
    "        else:\n",
    "            # MR modalities: Percentile normalization [p1, p99] \u2192 [0, 255]\n",
    "            p1, p99 = np.percentile(arr, [1, 99])\n",
    "            if p99 > p1:\n",
    "                arr = np.clip(arr, p1, p99)\n",
    "                arr = (arr - p1) / (p99 - p1)\n",
    "                arr = (arr * 255).astype(np.uint8)\n",
    "            else:\n",
    "                # Fallback: min-max normalization\n",
    "                img_min, img_max = arr.min(), arr.max()\n",
    "                if img_max > img_min:\n",
    "                    arr = (arr - img_min) / (img_max - img_min)\n",
    "                    arr = (arr * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    arr = np.zeros_like(arr, dtype=np.uint8)\n",
    "        \n",
    "        # Handle multi-frame DICOMs\n",
    "        if arr.ndim == 3:  # Multi-frame DICOM\n",
    "            for frame in arr:\n",
    "                frame_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                resized.append(frame_resized)\n",
    "        else:  # Single-frame DICOM\n",
    "            arr = cv2.resize(arr, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "            resized.append(arr)\n",
    "            \n",
    "    if len(resized) == 0:\n",
    "        # fallback to zeros to avoid crashes (rare)\n",
    "        vol = np.zeros((1, IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "    else:\n",
    "        vol = np.stack(resized, axis=0)  # [N,H,W] uint8\n",
    "    \n",
    "    return vol  # [N,H,W] uint8 - matching training data format\n",
    "\n",
    "def take_window_from_volume(vol_nhw: np.ndarray, center_idx: int, offsets=OFFSETS) -> np.ndarray:\n",
    "    # vol_nhw: [N,H,W] uint8\n",
    "    N = vol_nhw.shape[0]\n",
    "    idxs = [min(max(0, center_idx + o), N - 1) for o in offsets]\n",
    "    win = vol_nhw[idxs, :, :]              # [len(offsets),H,W]\n",
    "    return win.astype(np.float32, copy=False)  # Convert to float32 for model input\n",
    "\n",
    "def window_to_full_and_roi(win_chw: np.ndarray, coords: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # No coords on test \u2192 identical streams\n",
    "    return win_chw, win_chw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Model loading and discovery =========\n",
    "_ckpt_cache = None\n",
    "_models = None\n",
    "\n",
    "def discover_checkpoints() -> List[Tuple[str, str]]:\n",
    "    # Returns list of (arch_name, path)\n",
    "    found: List[Tuple[str, str]] = []\n",
    "    for base in CANDIDATE_MODEL_DIRS:\n",
    "        if not os.path.isdir(base):\n",
    "            continue\n",
    "        for root, _, files in os.walk(base):\n",
    "            for f in files:\n",
    "                if f.endswith('.pth') and 'tf_efficientnet_b0' in f and 'fold' in f and ('best' in f or 'final' in f):\n",
    "                    arch = 'tf_efficientnet_b0'  # Fixed architecture\n",
    "                    found.append((arch, os.path.join(root, f)))\n",
    "    # stable ordering\n",
    "    found.sort(key=lambda x: x[1])\n",
    "    return found\n",
    "\n",
    "def load_hybrid_model(arch_name: str, weight_path: str) -> nn.Module:\n",
    "    model = HybridAneurysmModel(base_model_name=arch_name, num_classes=len(LABEL_COLS))\n",
    "    state = torch.load(weight_path, map_location=DEVICE)\n",
    "    \n",
    "    # Handle different state dict formats\n",
    "    if isinstance(state, dict) and 'model_state_dict' in state:\n",
    "        state = state['model_state_dict']\n",
    "    elif isinstance(state, dict) and any(k.startswith('module.') for k in state.keys()):\n",
    "        state = {k.replace('module.', '', 1): v for k, v in state.items()}\n",
    "    \n",
    "    # Fix layer name mismatch: classifier -> fc\n",
    "    if isinstance(state, dict):\n",
    "        state = {k.replace('classifier.', 'fc.') if k.startswith('classifier.') else k: v for k, v in state.items()}\n",
    "    \n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval().to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def get_models() -> List[Tuple[str, nn.Module]]:\n",
    "    global _ckpt_cache, _models\n",
    "    if _models is not None:\n",
    "        return _models\n",
    "    _ckpt_cache = discover_checkpoints()\n",
    "    if not _ckpt_cache:\n",
    "        raise FileNotFoundError('No model checkpoints found. Make sure model dataset is attached.')\n",
    "    mods: List[Tuple[str, nn.Module]] = []\n",
    "    for arch, path in _ckpt_cache:\n",
    "        try:\n",
    "            m = load_hybrid_model(arch, path)\n",
    "            mods.append((arch, m))\n",
    "            print(f\"Loaded model: {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {path}: {e}\")\n",
    "            continue\n",
    "    if not mods:\n",
    "        raise RuntimeError('Failed to load any checkpoints from discovered files.')\n",
    "    _models = mods\n",
    "    print(f\"Loaded {len(_models)} models total\")\n",
    "    return _models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Inference pipeline =========\n",
    "@torch.no_grad()\n",
    "def predict_series_probs(dicoms) -> np.ndarray:\n",
    "    models = get_models()\n",
    "    # Build normalized volume [N,H,W] uint8 (matching training data format)\n",
    "    vol = series_to_tensor_chw(dicoms)\n",
    "    N = vol.shape[0]\n",
    "    # Prepare coords zeros on test\n",
    "    coords = np.zeros((N, 2), dtype=np.float32)\n",
    "\n",
    "    all_model_probs = []\n",
    "    for _, model in models:\n",
    "        batch_full, batch_roi, batch_coords = [], [], []\n",
    "        probs_accum = []\n",
    "        for c in range(N):\n",
    "            win = take_window_from_volume(vol, c, OFFSETS)   # [C,H,W] float32\n",
    "            win_chw = np.transpose(win, (0, 1, 2))           # still [C,H,W]\n",
    "            full_chw, roi_chw = window_to_full_and_roi(win_chw, coords[c])\n",
    "            batch_full.append(full_chw)\n",
    "            batch_roi.append(roi_chw)\n",
    "            batch_coords.append(coords[c])\n",
    "            # flush by batch\n",
    "            if len(batch_full) == BATCH_SIZE or c == N - 1:\n",
    "                xb_full = torch.from_numpy(np.stack(batch_full).astype(np.float32)).to(DEVICE)\n",
    "                xb_roi  = torch.from_numpy(np.stack(batch_roi).astype(np.float32)).to(DEVICE)\n",
    "                cb      = torch.from_numpy(np.stack(batch_coords).astype(np.float32)).to(DEVICE)\n",
    "                logits = model(xb_full, xb_roi, cb)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                probs_accum.append(probs)\n",
    "                batch_full.clear(); batch_roi.clear(); batch_coords.clear()\n",
    "        probs_all = np.concatenate(probs_accum, axis=0) if probs_accum else np.zeros((1, len(LABEL_COLS)), dtype=np.float32)\n",
    "        if AGGREGATE == 'max':\n",
    "            series_prob = probs_all.max(axis=0)\n",
    "        elif AGGREGATE == 'mean':\n",
    "            series_prob = probs_all.mean(axis=0)\n",
    "        else:  # topk_mean\n",
    "            k = max(1, N // 5)\n",
    "            series_prob = np.sort(probs_all, axis=0)[-k:].mean(axis=0)\n",
    "        all_model_probs.append(series_prob)\n",
    "        # free memory between models\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    # ensemble (probability average)\n",
    "    return np.mean(np.stack(all_model_probs, axis=0), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Kaggle-required predict function =========\n",
    "def predict(series_path: str) -> pl.DataFrame | pd.DataFrame:\n",
    "    series_id = os.path.basename(series_path)\n",
    "\n",
    "    # Collect all DICOM files\n",
    "    filepaths = []\n",
    "    for root, _, files in os.walk(series_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.dcm'):\n",
    "                filepaths.append(os.path.join(root, f))\n",
    "    \n",
    "    if not filepaths:\n",
    "        # Return zeros if no DICOMs found\n",
    "        zeros = [[series_id] + [0.0] * len(LABEL_COLS)]\n",
    "        predictions = pl.DataFrame(data=zeros, schema=[ID_COL, *LABEL_COLS], orient='row')\n",
    "        return predictions.drop(ID_COL)\n",
    "    \n",
    "    # Sort DICOMs and perform inference\n",
    "    dicoms = sort_dicom_slices(filepaths)\n",
    "    probs = predict_series_probs(dicoms)\n",
    "\n",
    "    # Build output (one row)\n",
    "    data = [[series_id] + probs.tolist()]\n",
    "    predictions = pl.DataFrame(data=data, schema=[ID_COL, *LABEL_COLS], orient='row')\n",
    "\n",
    "    # Required cleanup to avoid disk pressure\n",
    "    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n",
    "\n",
    "    # Server expects features only (without ID_COL)\n",
    "    return predictions.drop(ID_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading and basic functionality\n",
    "print(\"Testing model discovery...\")\n",
    "try:\n",
    "    checkpoints = discover_checkpoints()\n",
    "    print(f\"Found {len(checkpoints)} model checkpoints:\")\n",
    "    for arch, path in checkpoints:\n",
    "        print(f\"  - {arch}: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error discovering models: {e}\")\n",
    "\n",
    "# Test if we're in competition mode\n",
    "if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Running in test mode - not competition submission\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Start RSNA inference server =========\n",
    "inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # Competition mode - serve predictions\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Test mode - run local gateway\n",
    "    inference_server.run_local_gateway()\n",
    "    if os.path.exists('/kaggle/working/submission.parquet'):\n",
    "        print(\"Submission file created successfully\")\n",
    "        submission_df = pl.read_parquet('/kaggle/working/submission.parquet')\n",
    "        print(f\"Submission shape: {submission_df.shape}\")\n",
    "        print(submission_df.head())\n",
    "    else:\n",
    "        print(\"No submission file generated\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}