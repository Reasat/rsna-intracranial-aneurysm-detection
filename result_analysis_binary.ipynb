{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c97c9b",
   "metadata": {},
   "source": [
    "# RSNA 2025 Intracranial Aneurysm Detection - Binary Result Analysis\n",
    "\n",
    "This notebook performs comprehensive cross-fold analysis of misclassifications from the 5-fold CV training using binary models with a modular engine-based architecture.\n",
    "\n",
    "## Architecture\n",
    "- **InferenceEngine**: Handles data loading, binary model loading, and prediction collection\n",
    "- **AnalysisEngine**: Performs binary misclassification analysis and hard sample identification\n",
    "- **VisualizationEngine**: Creates comprehensive visualizations and plots for binary classification\n",
    "\n",
    "## Analysis Framework\n",
    "- **Out-of-Fold (OOF) Predictions**: Uses true OOF predictions for each sample (no data leakage)\n",
    "- **Binary Classification Analysis**: Focuses on aneurysm present/absent classification\n",
    "- **Hard Sample Identification**: Identifies challenging cases for binary classification\n",
    "\n",
    "## Key Features\n",
    "- Modular, separated engine architecture\n",
    "- Comprehensive binary misclassification analysis\n",
    "- Per-modality error breakdown\n",
    "- Hard sample case studies\n",
    "- Interactive visualizations\n",
    "- Actionable insights for binary model improvement\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from model import BinaryAneurysmModel, create_binary_model\n",
    "from config import Config\n",
    "from utils import ID_COL, load_cached_volume, take_window, valid_coords\n",
    "from analysis import InferenceEngine, AnalysisEngine, VisualizationEngine\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293cd30",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for binary models\n",
    "EXPERIMENT_DIR = \"models/2025-09-15-05-27-19\"  # Binary model experiment directory\n",
    "NUM_FOLDS = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load experiment configuration\n",
    "with open(f\"{EXPERIMENT_DIR}/used_config.yaml\", 'r') as f:\n",
    "    import yaml\n",
    "    config_dict = yaml.safe_load(f)\n",
    "\n",
    "# Create Config object\n",
    "path = f\"configs/train_config_binary.yaml\"\n",
    "config = Config(path) # load binary config\n",
    "\n",
    "# Override with experiment config\n",
    "config.architecture = config_dict['model']['architecture']\n",
    "config.img_size = config_dict['data']['img_size']\n",
    "config.window_offsets = config_dict['data']['window_offsets']\n",
    "config.roi_box_fraction = config_dict['data']['roi_box_fraction']\n",
    "config.cache_dir = config_dict['paths']['cache_dir']\n",
    "config.device = DEVICE\n",
    "\n",
    "# CRITICAL: Override num_classes for binary classification\n",
    "config.num_classes = 1  # Binary classification (aneurysm present/absent)\n",
    "\n",
    "print(f\"Binary Experiment: {EXPERIMENT_DIR}\")\n",
    "print(f\"Architecture: {config.architecture}\")\n",
    "print(f\"Image size: {config.img_size}\")\n",
    "print(f\"Num classes: {config.num_classes}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fb405",
   "metadata": {},
   "source": [
    "## 🔧 Aggregation Method Configuration\n",
    "\n",
    "The inference engine now supports two aggregation methods for combining predictions across all slices in a volume:\n",
    "\n",
    "### **MAX Aggregation (Default)**\n",
    "- **Method**: Takes the maximum prediction across all slices\n",
    "- **Use Case**: High sensitivity - captures any slice with high aneurysm probability\n",
    "- **Best For**: Detection tasks where missing an aneurysm is costly\n",
    "- **Example**: For predictions [0.1, 0.3, 0.7, 0.2, 0.9] → Result: 0.9\n",
    "\n",
    "### **MEAN Aggregation**\n",
    "- **Method**: Takes the average prediction across all slices\n",
    "- **Use Case**: Balanced approach - considers overall volume characteristics\n",
    "- **Best For**: Classification tasks where overall volume characteristics matter\n",
    "- **Example**: For predictions [0.1, 0.3, 0.7, 0.2, 0.9] → Result: 0.44\n",
    "\n",
    "### **Configuration**\n",
    "Change the `aggregation_method` variable in the next cell to switch between methods:\n",
    "- `aggregation_method = \"max\"` for maximum aggregation\n",
    "- `aggregation_method = \"mean\"` for mean aggregation\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316f440",
   "metadata": {},
   "source": [
    "## 🔧 Binary Inference Engine Execution\n",
    "\n",
    "The following section handles data loading, binary model loading, and prediction collection using the InferenceEngine.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff760043",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom InferenceEngine for binary models\n",
    "class BinaryInferenceEngine(InferenceEngine):\n",
    "    \"\"\"Custom InferenceEngine for binary models\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_dir: str, config: Config, aggregation: str = \"max\"):\n",
    "        super().__init__(experiment_dir, config, aggregation)\n",
    "        self.num_classes = 1  # Binary classification\n",
    "    \n",
    "    def load_fold_models(self):\n",
    "        \"\"\"Load all fold models for binary classification\"\"\"\n",
    "        for fold in range(5):\n",
    "            model_path = f\"{self.experiment_dir}/tf_efficientnet_b0_fold{fold}_best.pth\"\n",
    "            if os.path.exists(model_path):\n",
    "                # Create binary model instead of hybrid model\n",
    "                model = create_binary_model(self.config)\n",
    "                state_dict = torch.load(model_path, map_location=self.device)\n",
    "                model.load_state_dict(state_dict)\n",
    "                model.to(self.device)\n",
    "                model.eval()\n",
    "                self.fold_models[fold] = model\n",
    "                print(f\"Loaded binary fold {fold} model\")\n",
    "            else:\n",
    "                print(f\"Warning: Binary model not found for fold {fold}\")\n",
    "\n",
    "# Create a custom AnalysisEngine for binary models\n",
    "class BinaryAnalysisEngine(AnalysisEngine):\n",
    "    \"\"\"Custom AnalysisEngine for binary models\"\"\"\n",
    "    \n",
    "    def __init__(self, inference_engine: BinaryInferenceEngine):\n",
    "        super().__init__(inference_engine)\n",
    "        self.num_classes = 1  # Binary classification\n",
    "    \n",
    "    def analyze_binary_misclassifications(self, true_labels_df: pd.DataFrame):\n",
    "        \"\"\"Analyze binary misclassifications for aneurysm present/absent classification\"\"\"\n",
    "        from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        class_name = 'Aneurysm Present'\n",
    "        class_analysis = {\n",
    "            'class_name': class_name,\n",
    "            'total_samples': 0,\n",
    "            'positive_samples': 0,\n",
    "            'misclassified_samples': [],\n",
    "            'false_positives': [],\n",
    "            'false_negatives': [],\n",
    "            'true_positives': [],\n",
    "            'true_negatives': [],\n",
    "            'confidence_distribution': {'correct': [], 'incorrect': []}\n",
    "        }\n",
    "        \n",
    "        # Collect predictions and true labels\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for sample_id in self.inference_engine.oof_predictions.keys():\n",
    "            if sample_id not in true_labels_df[ID_COL].values:\n",
    "                continue\n",
    "                \n",
    "            true_label = true_labels_df[true_labels_df[ID_COL] == sample_id][class_name].iloc[0]\n",
    "            oof_pred = self.inference_engine.oof_predictions[sample_id]  # Binary prediction [1]\n",
    "            \n",
    "            # Binary prediction is a 1-element array, so use index 0\n",
    "            pred_prob = oof_pred[0] if len(oof_pred) > 0 else 0.0\n",
    "            pred_binary = 1 if pred_prob >= 0.5 else 0\n",
    "            \n",
    "            class_analysis['total_samples'] += 1\n",
    "            if true_label == 1:\n",
    "                class_analysis['positive_samples'] += 1\n",
    "            \n",
    "            # Categorize predictions\n",
    "            if true_label == 1 and pred_binary == 1:\n",
    "                class_analysis['true_positives'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred_prob,\n",
    "                    'fold_predictions': [pred_prob]\n",
    "                })\n",
    "            elif true_label == 0 and pred_binary == 0:\n",
    "                class_analysis['true_negatives'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred_prob,\n",
    "                    'fold_predictions': [pred_prob]\n",
    "                })\n",
    "            elif true_label == 1 and pred_binary == 0:\n",
    "                class_analysis['false_negatives'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred_prob,\n",
    "                    'fold_predictions': [pred_prob]\n",
    "                })\n",
    "            elif true_label == 0 and pred_binary == 1:\n",
    "                class_analysis['false_positives'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred_prob,\n",
    "                    'fold_predictions': [pred_prob]\n",
    "                })\n",
    "            \n",
    "            # Track confidence distribution\n",
    "            if (true_label == 1 and pred_binary == 1) or (true_label == 0 and pred_binary == 0):\n",
    "                class_analysis['confidence_distribution']['correct'].append(pred_prob)\n",
    "            else:\n",
    "                class_analysis['confidence_distribution']['incorrect'].append(pred_prob)\n",
    "            \n",
    "            predictions.append(pred_prob)\n",
    "            true_labels.append(true_label)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(predictions) > 0 and len(set(true_labels)) > 1:\n",
    "            class_analysis['auc'] = roc_auc_score(true_labels, predictions)\n",
    "            \n",
    "            # Calculate precision, recall, F1 with binary predictions\n",
    "            binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "            class_analysis['precision'] = precision_score(true_labels, binary_preds, zero_division=0)\n",
    "            class_analysis['recall'] = recall_score(true_labels, binary_preds, zero_division=0)\n",
    "            class_analysis['f1_score'] = f1_score(true_labels, binary_preds, zero_division=0)\n",
    "        else:\n",
    "            class_analysis['auc'] = 0.0\n",
    "            class_analysis['precision'] = 0.0\n",
    "            class_analysis['recall'] = 0.0\n",
    "            class_analysis['f1_score'] = 0.0\n",
    "        \n",
    "        return {'Aneurysm Present': class_analysis}\n",
    "    \n",
    "    def identify_binary_hard_samples(self, true_labels_df: pd.DataFrame):\n",
    "        \"\"\"Identify hard samples for binary classification\"\"\"\n",
    "        hard_samples = {\n",
    "            'high_confidence_errors': [],\n",
    "            'low_confidence_correct': [],\n",
    "            'borderline_predictions': [],\n",
    "            'modality_specific_errors': []\n",
    "        }\n",
    "        \n",
    "        for sample_id in self.inference_engine.oof_predictions.keys():\n",
    "            if sample_id not in true_labels_df[ID_COL].values:\n",
    "                continue\n",
    "                \n",
    "            # Get true label for \"Aneurysm Present\"\n",
    "            true_label = true_labels_df[true_labels_df[ID_COL] == sample_id]['Aneurysm Present'].iloc[0]\n",
    "            oof_pred = self.inference_engine.oof_predictions[sample_id]  # Binary prediction [1]\n",
    "            \n",
    "            # Binary prediction is a 1-element array, so use index 0\n",
    "            pred_prob = oof_pred[0] if len(oof_pred) > 0 else 0.0\n",
    "            pred_binary = 1 if pred_prob >= 0.5 else 0\n",
    "            \n",
    "            # Determine if prediction is correct\n",
    "            is_correct = (true_label == 1) == (pred_binary == 1)\n",
    "            \n",
    "            # Categorize hard samples\n",
    "            if not is_correct:\n",
    "                # High confidence errors (wrong prediction with high confidence)\n",
    "                if pred_prob > 0.8 or pred_prob < 0.2:\n",
    "                    hard_samples['high_confidence_errors'].append({\n",
    "                        'sample_id': sample_id,\n",
    "                        'true_label': true_label,\n",
    "                        'prediction': pred_prob,\n",
    "                        'confidence': max(pred_prob, 1 - pred_prob)\n",
    "                    })\n",
    "            else:\n",
    "                # Low confidence correct predictions\n",
    "                if 0.4 <= pred_prob <= 0.6:\n",
    "                    hard_samples['low_confidence_correct'].append({\n",
    "                        'sample_id': sample_id,\n",
    "                        'true_label': true_label,\n",
    "                        'prediction': pred_prob,\n",
    "                        'confidence': min(pred_prob, 1 - pred_prob)\n",
    "                    })\n",
    "            \n",
    "            # Borderline predictions (close to threshold)\n",
    "            if 0.3 <= pred_prob <= 0.7:\n",
    "                hard_samples['borderline_predictions'].append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'true_label': true_label,\n",
    "                    'prediction': pred_prob,\n",
    "                    'is_correct': is_correct\n",
    "                })\n",
    "        \n",
    "        return hard_samples\n",
    "    \n",
    "    def analyze_modality_binary_performance(self, true_labels_df: pd.DataFrame, sample_modalities: dict):\n",
    "        \"\"\"Analyze binary model performance across different modalities\"\"\"\n",
    "        from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        modality_results = {}\n",
    "        modalities = list(set(sample_modalities.values()))\n",
    "        \n",
    "        for modality in modalities:\n",
    "            modality_analysis = {\n",
    "                'modality': modality,\n",
    "                'total_samples': 0,\n",
    "                'aneurysm_present_samples': 0,\n",
    "                'per_class_analysis': {},\n",
    "                'overall_metrics': {},\n",
    "                'confusion_matrix': {'true_positives': 0, 'true_negatives': 0, 'false_positives': 0, 'false_negatives': 0}\n",
    "            }\n",
    "            \n",
    "            # Filter samples by modality\n",
    "            modality_samples = [sid for sid, mod in sample_modalities.items() \n",
    "                              if mod == modality and sid in self.inference_engine.oof_predictions]\n",
    "            \n",
    "            modality_analysis['total_samples'] = len(modality_samples)\n",
    "            \n",
    "            # Analyze \"Aneurysm Present\" class for this modality\n",
    "            class_name = 'Aneurysm Present'\n",
    "            class_analysis = {\n",
    "                'class_name': class_name,\n",
    "                'total_samples': 0,\n",
    "                'positive_samples': 0,\n",
    "                'predictions': [],\n",
    "                'true_labels': [],\n",
    "                'false_positives': [],\n",
    "                'false_negatives': [],\n",
    "                'true_positives': [],\n",
    "                'true_negatives': []\n",
    "            }\n",
    "            \n",
    "            for sample_id in modality_samples:\n",
    "                if sample_id not in true_labels_df[ID_COL].values:\n",
    "                    continue\n",
    "                \n",
    "                true_label = true_labels_df[true_labels_df[ID_COL] == sample_id][class_name].iloc[0]\n",
    "                oof_pred = self.inference_engine.oof_predictions[sample_id]  # Binary prediction [1]\n",
    "                \n",
    "                # Binary prediction is a 1-element array, so use index 0\n",
    "                pred_score = oof_pred[0] if len(oof_pred) > 0 else 0.0\n",
    "                pred_binary = 1 if pred_score >= 0.5 else 0\n",
    "                \n",
    "                class_analysis['total_samples'] += 1\n",
    "                class_analysis['predictions'].append(pred_score)\n",
    "                class_analysis['true_labels'].append(true_label)\n",
    "                \n",
    "                if true_label == 1:\n",
    "                    class_analysis['positive_samples'] += 1\n",
    "                    modality_analysis['aneurysm_present_samples'] += 1\n",
    "                    \n",
    "                    if pred_binary == 1:\n",
    "                        class_analysis['true_positives'].append(sample_id)\n",
    "                        modality_analysis['confusion_matrix']['true_positives'] += 1\n",
    "                    else:\n",
    "                        class_analysis['false_negatives'].append(sample_id)\n",
    "                        modality_analysis['confusion_matrix']['false_negatives'] += 1\n",
    "                else:\n",
    "                    if pred_binary == 1:\n",
    "                        class_analysis['false_positives'].append(sample_id)\n",
    "                        modality_analysis['confusion_matrix']['false_positives'] += 1\n",
    "                    else:\n",
    "                        class_analysis['true_negatives'].append(sample_id)\n",
    "                        modality_analysis['confusion_matrix']['true_negatives'] += 1\n",
    "            \n",
    "            # Calculate metrics for this modality\n",
    "            if len(class_analysis['predictions']) > 0 and len(set(class_analysis['true_labels'])) > 1:\n",
    "                class_analysis['auc'] = roc_auc_score(class_analysis['true_labels'], class_analysis['predictions'])\n",
    "                \n",
    "                binary_preds = [1 if p >= 0.5 else 0 for p in class_analysis['predictions']]\n",
    "                class_analysis['precision'] = precision_score(class_analysis['true_labels'], binary_preds, zero_division=0)\n",
    "                class_analysis['recall'] = recall_score(class_analysis['true_labels'], binary_preds, zero_division=0)\n",
    "                class_analysis['f1_score'] = f1_score(class_analysis['true_labels'], binary_preds, zero_division=0)\n",
    "                \n",
    "                # Overall metrics for modality\n",
    "                modality_analysis['overall_metrics'] = {\n",
    "                    'auc': class_analysis['auc'],\n",
    "                    'precision': class_analysis['precision'],\n",
    "                    'recall': class_analysis['recall'],\n",
    "                    'f1': class_analysis['f1_score']\n",
    "                }\n",
    "            else:\n",
    "                class_analysis['auc'] = 0.0\n",
    "                class_analysis['precision'] = 0.0\n",
    "                class_analysis['recall'] = 0.0\n",
    "                class_analysis['f1_score'] = 0.0\n",
    "                \n",
    "                modality_analysis['overall_metrics'] = {\n",
    "                    'auc': 0.0,\n",
    "                    'precision': 0.0,\n",
    "                    'recall': 0.0,\n",
    "                    'f1': 0.0\n",
    "                }\n",
    "            \n",
    "            modality_analysis['per_class_analysis'][class_name] = class_analysis\n",
    "            modality_results[modality] = modality_analysis\n",
    "        \n",
    "        return modality_results\n",
    "    \n",
    "    def identify_modality_binary_hard_samples(self, true_labels_df: pd.DataFrame, sample_modalities: dict):\n",
    "        \"\"\"Identify modality-specific hard samples for binary classification\"\"\"\n",
    "        modality_hard_samples = {}\n",
    "        modalities = list(set(sample_modalities.values()))\n",
    "        \n",
    "        for modality in modalities:\n",
    "            modality_hard_samples[modality] = {\n",
    "                'high_confidence_errors': [],\n",
    "                'low_confidence_correct': [],\n",
    "                'borderline_predictions': []\n",
    "            }\n",
    "            \n",
    "            # Filter samples by modality\n",
    "            modality_samples = [sid for sid, mod in sample_modalities.items() \n",
    "                              if mod == modality and sid in self.inference_engine.oof_predictions]\n",
    "            \n",
    "            for sample_id in modality_samples:\n",
    "                if sample_id not in true_labels_df[ID_COL].values:\n",
    "                    continue\n",
    "                \n",
    "                true_label = true_labels_df[true_labels_df[ID_COL] == sample_id]['Aneurysm Present'].iloc[0]\n",
    "                oof_pred = self.inference_engine.oof_predictions[sample_id]  # Binary prediction [1]\n",
    "                \n",
    "                # Binary prediction is a 1-element array, so use index 0\n",
    "                pred_prob = oof_pred[0] if len(oof_pred) > 0 else 0.0\n",
    "                pred_binary = 1 if pred_prob >= 0.5 else 0\n",
    "                \n",
    "                # Determine if prediction is correct\n",
    "                is_correct = (true_label == 1) == (pred_binary == 1)\n",
    "                \n",
    "                # Categorize hard samples\n",
    "                if not is_correct:\n",
    "                    # High confidence errors\n",
    "                    if pred_prob > 0.8 or pred_prob < 0.2:\n",
    "                        modality_hard_samples[modality]['high_confidence_errors'].append({\n",
    "                            'sample_id': sample_id,\n",
    "                            'true_label': true_label,\n",
    "                            'prediction': pred_prob,\n",
    "                            'confidence': max(pred_prob, 1 - pred_prob)\n",
    "                        })\n",
    "                else:\n",
    "                    # Low confidence correct predictions\n",
    "                    if 0.4 <= pred_prob <= 0.6:\n",
    "                        modality_hard_samples[modality]['low_confidence_correct'].append({\n",
    "                            'sample_id': sample_id,\n",
    "                            'true_label': true_label,\n",
    "                            'prediction': pred_prob,\n",
    "                            'confidence': min(pred_prob, 1 - pred_prob)\n",
    "                        })\n",
    "                \n",
    "                # Borderline predictions\n",
    "                if 0.3 <= pred_prob <= 0.7:\n",
    "                    modality_hard_samples[modality]['borderline_predictions'].append({\n",
    "                        'sample_id': sample_id,\n",
    "                        'true_label': true_label,\n",
    "                        'prediction': pred_prob,\n",
    "                        'is_correct': is_correct\n",
    "                    })\n",
    "        \n",
    "        return modality_hard_samples\n",
    "\n",
    "# Initialize engines (adapted for binary models)\n",
    "# Choose aggregation method: \"max\" or \"mean\"\n",
    "aggregation_method = \"mean\"  # Change to \"mean\" for mean aggregation\n",
    "print(f\"Using {aggregation_method} aggregation for inference\")\n",
    "\n",
    "inference_engine = BinaryInferenceEngine(EXPERIMENT_DIR, config, aggregation=aggregation_method)\n",
    "analysis_engine = BinaryAnalysisEngine(inference_engine)\n",
    "visualization_engine = VisualizationEngine(inference_engine)\n",
    "# Override num_classes for binary classification\n",
    "visualization_engine.num_classes = 1\n",
    "\n",
    "# Load fold assignments\n",
    "train_csv_path = config_dict['paths']['train_csv']\n",
    "inference_engine.load_fold_assignments(train_csv_path)\n",
    "\n",
    "# Load binary models\n",
    "inference_engine.load_fold_models()\n",
    "\n",
    "# Get sample IDs for analysis (use a subset for testing)\n",
    "all_sample_ids = list(inference_engine.fold_assignments.keys())\n",
    "print(f\"Total samples available: {len(all_sample_ids)}\")\n",
    "\n",
    "# For testing, use a subset (remove this for full analysis)\n",
    "test_sample_ids = np.random.RandomState(42).choice(all_sample_ids, size=1000, replace=False).tolist()\n",
    "print(f\"Using {len(test_sample_ids)} samples for binary analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect OOF predictions for binary classification\n",
    "inference_engine.collect_oof_predictions(test_sample_ids)\n",
    "\n",
    "print(f\"Collected OOF predictions for {len(inference_engine.oof_predictions)} samples\")\n",
    "print(f\"Average predictions per sample: {np.mean([len(preds) for preds in inference_engine.oof_predictions.values()]):.1f}\")\n",
    "\n",
    "# Load true labels for analysis\n",
    "true_labels_df = pd.read_csv(train_csv_path)\n",
    "print(f\"Loaded true labels for {len(true_labels_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee98c16",
   "metadata": {},
   "source": [
    "## 📊 Binary Analysis Engine Execution\n",
    "\n",
    "The following section uses the AnalysisEngine to perform binary misclassification analysis and identify hard samples.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed43edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive binary analysis\n",
    "print(\"🔍 Starting Binary Cross-Fold Analysis...\")\n",
    "\n",
    "# 1. Binary misclassification analysis\n",
    "print(\"📊 Analyzing binary misclassifications...\")\n",
    "binary_analysis = analysis_engine.analyze_binary_misclassifications(true_labels_df)\n",
    "\n",
    "# 2. Hard sample identification for binary classification\n",
    "print(\"🎯 Identifying binary hard samples...\")\n",
    "hard_samples = analysis_engine.identify_binary_hard_samples(true_labels_df)\n",
    "\n",
    "print(\"✅ Binary analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print binary analysis results (focusing on \"Aneurysm Present\" class)\n",
    "aneurysm_present_analysis = binary_analysis['Aneurysm Present']\n",
    "print(\"Binary Analysis Results (Aneurysm Present class):\")\n",
    "print(f\"Total samples: {aneurysm_present_analysis['total_samples']}\")\n",
    "print(f\"Positive samples (aneurysm present): {aneurysm_present_analysis['positive_samples']}\")\n",
    "print(f\"Negative samples (no aneurysm): {aneurysm_present_analysis['total_samples'] - aneurysm_present_analysis['positive_samples']}\")\n",
    "print(f\"False Negatives: {len(aneurysm_present_analysis['false_negatives'])}\")\n",
    "print(f\"False Positives: {len(aneurysm_present_analysis['false_positives'])}\")\n",
    "print(f\"True Positives: {len(aneurysm_present_analysis['true_positives'])}\")\n",
    "print(f\"True Negatives: {len(aneurysm_present_analysis['true_negatives'])}\")\n",
    "print(f\"Overall AUC: {aneurysm_present_analysis['auc']:.3f}\")\n",
    "print(f\"Precision: {aneurysm_present_analysis['precision']:.3f}\")\n",
    "print(f\"Recall: {aneurysm_present_analysis['recall']:.3f}\")\n",
    "print(f\"F1 Score: {aneurysm_present_analysis['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b39495",
   "metadata": {},
   "source": [
    "## 📈 Binary Visualization Engine Execution\n",
    "\n",
    "The following section uses the VisualizationEngine to create comprehensive visualizations of the binary analysis results.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary visualizations (moved to 4-modality section below)\n",
    "print(\"📈 Binary visualizations will be generated in the 4-modality analysis section...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef4e4a",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive binary results summary\n",
    "print(\"=\" * 80)\n",
    "print(\"📋 BINARY CROSS-FOLD ANALYSIS RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics\n",
    "total_samples = len(inference_engine.oof_predictions)\n",
    "print(f\"\\n📊 Overall Statistics:\")\n",
    "print(f\"  • Total samples analyzed: {total_samples}\")\n",
    "print(f\"  • Average OOF predictions per sample: {np.mean([len(preds) for preds in inference_engine.oof_predictions.values()]):.1f}\")\n",
    "\n",
    "# Binary classification summary (focusing on Aneurysm Present class)\n",
    "aneurysm_analysis = binary_analysis['Aneurysm Present']\n",
    "print(f\"\\n🎯 Binary Classification Summary (Aneurysm Present):\")\n",
    "print(f\"  • Total samples: {aneurysm_analysis['total_samples']}\")\n",
    "print(f\"  • Positive samples (aneurysm present): {aneurysm_analysis['positive_samples']} ({aneurysm_analysis['positive_samples']/aneurysm_analysis['total_samples']*100:.1f}%)\")\n",
    "print(f\"  • Negative samples (no aneurysm): {aneurysm_analysis['total_samples'] - aneurysm_analysis['positive_samples']} ({(aneurysm_analysis['total_samples'] - aneurysm_analysis['positive_samples'])/aneurysm_analysis['total_samples']*100:.1f}%)\")\n",
    "print(f\"  • False Negatives: {len(aneurysm_analysis['false_negatives'])}\")\n",
    "print(f\"  • False Positives: {len(aneurysm_analysis['false_positives'])}\")\n",
    "print(f\"  • True Positives: {len(aneurysm_analysis['true_positives'])}\")\n",
    "print(f\"  • True Negatives: {len(aneurysm_analysis['true_negatives'])}\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\n📈 Performance Metrics:\")\n",
    "print(f\"  • Overall AUC: {aneurysm_analysis['auc']:.3f}\")\n",
    "print(f\"  • Precision: {aneurysm_analysis['precision']:.3f}\")\n",
    "print(f\"  • Recall: {aneurysm_analysis['recall']:.3f}\")\n",
    "print(f\"  • F1 Score: {aneurysm_analysis['f1_score']:.3f}\")\n",
    "\n",
    "# Hard samples summary\n",
    "print(f\"\\n🎯 Hard Sample Summary:\")\n",
    "for hard_type, samples in hard_samples.items():\n",
    "    print(f\"  • {hard_type}: {len(samples)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Binary Analysis Complete! Check visualizations above for detailed insights.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952c7c5",
   "metadata": {},
   "source": [
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da823c7",
   "metadata": {},
   "source": [
    "## 🔬 4-Modality Binary Analysis\n",
    "\n",
    "This section demonstrates the 4-modality analysis capability for binary classification:\n",
    "- **CTA** (CT Angiography)\n",
    "- **MRA** (MR Angiography) \n",
    "- **MRI T2** (T2-weighted MRI)\n",
    "- **MRI T1post** (T1-weighted post-contrast MRI)\n",
    "\n",
    "The analysis extracts modality information from CSV and performs comprehensive 4-way comparisons for binary classification.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract modality mapping for 4-modality binary analysis\n",
    "from analysis import extract_modality_mapping\n",
    "\n",
    "# Get sample IDs that have predictions\n",
    "sample_ids = list(inference_engine.oof_predictions.keys())\n",
    "print(f\"Analyzing {len(sample_ids)} samples with binary predictions\")\n",
    "\n",
    "# Extract modality mapping from CSV\n",
    "modality_mapping = extract_modality_mapping(sample_ids, train_csv_path)\n",
    "\n",
    "# Show the 4-modality distribution\n",
    "print(f\"\\n4-Modality Distribution:\")\n",
    "for modality, count in sorted(modality_mapping.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {modality}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4-modality binary analysis\n",
    "print(\"🔬 Starting 4-Modality Binary Analysis...\")\n",
    "\n",
    "# 1. Modality-specific binary performance analysis\n",
    "print(\"📊 Analyzing binary performance across 4 modalities...\")\n",
    "modality_analysis = analysis_engine.analyze_modality_binary_performance(\n",
    "    true_labels_df, \n",
    "    sample_modalities=modality_mapping\n",
    ")\n",
    "\n",
    "# 2. Modality-specific binary hard sample identification\n",
    "print(\"🎯 Identifying modality-specific binary hard samples...\")\n",
    "modality_hard_samples = analysis_engine.identify_modality_binary_hard_samples(\n",
    "    true_labels_df,\n",
    "    sample_modalities=modality_mapping\n",
    ")\n",
    "\n",
    "print(\"✅ 4-Modality binary analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 4-modality binary performance summary (focusing on Aneurysm Present class)\n",
    "print(\"=\" * 80)\n",
    "print(\"📋 4-MODALITY BINARY ANALYSIS RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for modality, analysis in modality_analysis.items():\n",
    "    total_samples = analysis['total_samples']\n",
    "    aneurysm_samples = analysis['aneurysm_present_samples']\n",
    "    \n",
    "    # Get Aneurysm Present class analysis\n",
    "    aneurysm_class_analysis = analysis['per_class_analysis']['Aneurysm Present']\n",
    "    auc = aneurysm_class_analysis.get('auc', 0.0)\n",
    "    precision = aneurysm_class_analysis.get('precision', 0.0)\n",
    "    recall = aneurysm_class_analysis.get('recall', 0.0)\n",
    "    f1 = aneurysm_class_analysis.get('f1_score', 0.0)\n",
    "    \n",
    "    print(f\"\\n🔬 {modality}:\")\n",
    "    print(f\"  • Total samples: {total_samples}\")\n",
    "    print(f\"  • Aneurysm present: {aneurysm_samples} ({aneurysm_samples/total_samples*100:.1f}%)\")\n",
    "    print(f\"  • AUC: {auc:.3f}\")\n",
    "    print(f\"  • Precision: {precision:.3f}\")\n",
    "    print(f\"  • Recall: {recall:.3f}\")\n",
    "    print(f\"  • F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    # Show confusion matrix summary for Aneurysm Present class\n",
    "    tn = len(aneurysm_class_analysis['true_negatives'])\n",
    "    fp = len(aneurysm_class_analysis['false_positives'])\n",
    "    fn = len(aneurysm_class_analysis['false_negatives'])\n",
    "    tp = len(aneurysm_class_analysis['true_positives'])\n",
    "    print(f\"  • Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-modality AUCs\n",
    "print(\"📊 Creating per-modality AUC plots...\")\n",
    "\n",
    "def plot_modality_aucs(modality_analysis):\n",
    "    \"\"\"Plot per-modality AUCs for binary classification\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Extract modality names and AUCs\n",
    "    modalities = list(modality_analysis.keys())\n",
    "    aucs = []\n",
    "    \n",
    "    for mod in modalities:\n",
    "        aneurysm_class_analysis = modality_analysis[mod]['per_class_analysis']['Aneurysm Present']\n",
    "        auc = aneurysm_class_analysis.get('auc', 0.0)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(modalities, aucs, alpha=0.7, color='lightcoral', edgecolor='darkred', linewidth=1.5)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Per-Modality AUC Performance (Binary Classification)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Modality', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add AUC labels on bars\n",
    "    for i, (bar, auc) in enumerate(zip(bars, aucs)):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{auc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0.5 (random classifier)\n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random Classifier')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n📈 Per-Modality AUC Summary:\")\n",
    "    print(f\"  • Best performing modality: {modalities[aucs.index(max(aucs))]} (AUC: {max(aucs):.3f})\")\n",
    "    print(f\"  • Worst performing modality: {modalities[aucs.index(min(aucs))]} (AUC: {min(aucs):.3f})\")\n",
    "    print(f\"  • Average AUC across modalities: {np.mean(aucs):.3f}\")\n",
    "    print(f\"  • AUC standard deviation: {np.std(aucs):.3f}\")\n",
    "\n",
    "# Generate the plot\n",
    "plot_modality_aucs(modality_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 4-modality binary visualizations\n",
    "print(\"📈 Creating 4-modality binary visualizations...\")\n",
    "visualization_engine.create_visualizations(\n",
    "    per_class_analysis=binary_analysis,  # Use the per-class analysis\n",
    "    hard_samples=hard_samples,\n",
    "    true_labels_df=true_labels_df,\n",
    "    modality_analysis=modality_analysis,\n",
    "    modality_hard_samples=modality_hard_samples,\n",
    "    sample_modalities=modality_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfc43d",
   "metadata": {},
   "source": [
    "## 🔍 Detailed Binary Analysis\n",
    "\n",
    "This section provides detailed analysis of binary classification performance, including threshold analysis and error case studies.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed binary analysis\n",
    "print(\"🔍 Starting Detailed Binary Analysis...\")\n",
    "\n",
    "# Note: The current analysis engine doesn't have threshold_analysis and error_analysis methods\n",
    "# These would need to be implemented in the analysis.py file for full binary analysis\n",
    "print(\"📊 Threshold analysis and error case analysis methods not yet implemented in analysis engine\")\n",
    "print(\"🎯 Using available per-class analysis for binary classification insights\")\n",
    "\n",
    "print(\"✅ Detailed binary analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da89cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed analysis results (using available data)\n",
    "print(\"=\" * 60)\n",
    "print(\"📊 DETAILED BINARY ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the Aneurysm Present class analysis for detailed insights\n",
    "aneurysm_analysis = binary_analysis['Aneurysm Present']\n",
    "print(f\"Binary Classification Performance:\")\n",
    "print(f\"  • AUC: {aneurysm_analysis['auc']:.3f}\")\n",
    "print(f\"  • Precision: {aneurysm_analysis['precision']:.3f}\")\n",
    "print(f\"  • Recall: {aneurysm_analysis['recall']:.3f}\")\n",
    "print(f\"  • F1 Score: {aneurysm_analysis['f1_score']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 ERROR CASE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"False Negative cases: {len(aneurysm_analysis['false_negatives'])}\")\n",
    "print(f\"False Positive cases: {len(aneurysm_analysis['false_positives'])}\")\n",
    "print(f\"True Positive cases: {len(aneurysm_analysis['true_positives'])}\")\n",
    "print(f\"True Negative cases: {len(aneurysm_analysis['true_negatives'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7e928",
   "metadata": {},
   "source": [
    "## 📋 Final Summary and Recommendations\n",
    "\n",
    "This section provides a comprehensive summary of the binary analysis results and actionable recommendations for model improvement.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"📋 COMPREHENSIVE BINARY ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall performance (using Aneurysm Present class)\n",
    "aneurysm_analysis = binary_analysis['Aneurysm Present']\n",
    "print(\"\\n🎯 Overall Binary Performance (Aneurysm Present):\")\n",
    "print(f\"  • AUC: {aneurysm_analysis['auc']:.3f}\")\n",
    "print(f\"  • F1 Score: {aneurysm_analysis['f1_score']:.3f}\")\n",
    "print(f\"  • Precision: {aneurysm_analysis['precision']:.3f}\")\n",
    "print(f\"  • Recall: {aneurysm_analysis['recall']:.3f}\")\n",
    "\n",
    "# Modality performance ranking (by AUC for Aneurysm Present class)\n",
    "print(\"\\n🔬 Modality Performance Ranking (by AUC for Aneurysm Present):\")\n",
    "modality_aucs = []\n",
    "for mod, analysis in modality_analysis.items():\n",
    "    aneurysm_class_analysis = analysis['per_class_analysis']['Aneurysm Present']\n",
    "    auc = aneurysm_class_analysis.get('auc', 0.0)\n",
    "    modality_aucs.append((mod, auc))\n",
    "modality_aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (modality, auc) in enumerate(modality_aucs, 1):\n",
    "    print(f\"  {i}. {modality}: {auc:.3f}\")\n",
    "\n",
    "# Error analysis summary\n",
    "print(\"\\n❌ Error Analysis Summary:\")\n",
    "print(f\"  • False Negatives: {len(aneurysm_analysis['false_negatives'])} (missed aneurysms)\")\n",
    "print(f\"  • False Positives: {len(aneurysm_analysis['false_positives'])} (false alarms)\")\n",
    "print(f\"  • Error Rate: {(len(aneurysm_analysis['false_negatives']) + len(aneurysm_analysis['false_positives'])) / aneurysm_analysis['total_samples'] * 100:.1f}%\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n💡 Recommendations for Model Improvement:\")\n",
    "print(\"  1. Focus on reducing false negatives (missed aneurysms)\")\n",
    "print(\"  2. Consider modality-specific training strategies\")\n",
    "print(\"  3. Analyze hard samples for data augmentation opportunities\")\n",
    "print(\"  4. Optimize threshold based on clinical requirements\")\n",
    "print(\"  5. Consider ensemble approaches with 14-class models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ Binary Analysis Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
